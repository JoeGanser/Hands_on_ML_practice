{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ch11 Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 8: \n",
    "* Build a DNN with five hidden layers of 100 neurons each, with He intialization and ELU activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://www.pyimagesearch.com/2019/05/20/transfer-learning-with-keras-and-deep-learning/\n",
    "* https://medium.com/@14prakash/transfer-learning-using-keras-d804b2e04ef8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]\n",
    "\n",
    "X_train1 = X_train[y_train < 5]\n",
    "y_train1 = y_train[y_train < 5]\n",
    "X_valid1 = X_valid[y_valid < 5]\n",
    "y_valid1 = y_valid[y_valid < 5]\n",
    "X_test1 = X_test[y_test < 5]\n",
    "y_test1 = y_test[y_test < 5]\n",
    "\n",
    "\n",
    "X_train2 = X_train[y_train >= 5]\n",
    "y_train2 = y_train[y_train >= 5]\n",
    "X_test2 = X_test[y_test >= 5]\n",
    "y_test2 = y_test[y_test >=5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.initializers import he_uniform\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "\n",
    "n = 100 #number of neurons\n",
    "HE = he_uniform(seed=3)\n",
    "model = Sequential()\n",
    "for i in range(5):\n",
    "    model.add(Dense(n,activation='elu', kernel_initializer= HE))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "checkpoint = ModelCheckpoint('weights1_4.hdf5',monitor='val_loss', save_best_only=True)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "checkpoints_list = [checkpoint]\n",
    "history = model.fit(X_train1, y_train1,validation_split=0.2,epochs=10,\n",
    "            callbacks=checkpoints_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i in range(4):\n",
    "    key = list(history.history.keys())[i]\n",
    "    plt.plot(history.history[key],label=key)\n",
    "plt.legend()\n",
    "plt.title(\"History of loss and \\naccuracy as a function of epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Model(inputs=model.input,outputs=model.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_layer = 3\n",
    "for layer in model_2.layers[:N_layer]:\n",
    "    layer.trainable=False\n",
    "for layer in model_2.layers[N_layer:]:\n",
    "    layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint2 = ModelCheckpoint('weights1_5.hdf5',monitor='val_loss', save_best_only=True)\n",
    "model_2.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "checkpoints_list2 = [checkpoint2]\n",
    "\n",
    "model_2.fit(X_train2, y_train2,validation_split=0.2,epochs=10)#,\n",
    "           # callbacks=checkpoints_list2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try solving this problem using tensor flow instead of Keras\n",
    "\n",
    "To make a 5 layer Deep neural network with the number of neurons and intialization techniques required, we first must create a function that creates a set of layers\n",
    "\n",
    "This requires a few functionalities within the function itsself\n",
    "* Defining a scope\n",
    "* Creating a set of densely connected layers, each specifying the activation function, inputs, number of neurons,\n",
    "initializer and name and returns this\n",
    "\n",
    "Then after the function is used,\n",
    "\n",
    "We must create place holders to hold the data we input into the layer, and use our function that builds layers to act on these place holders\n",
    "\n",
    "then we define an output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]\n",
    "\n",
    "X_train1 = X_train[y_train < 5]\n",
    "y_train1 = y_train[y_train < 5]\n",
    "X_valid1 = X_valid[y_valid < 5]\n",
    "y_valid1 = y_valid[y_valid < 5]\n",
    "X_test1 = X_test[y_test < 5]\n",
    "y_test1 = y_test[y_test < 5]\n",
    "\n",
    "\n",
    "X_train2 = X_train[y_train >= 5]\n",
    "y_train2 = y_train[y_train>=5]\n",
    "X_test2 = X_test[y_test >= 5]\n",
    "y_test2 = y_test[y_test >=5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "def DeepNeuralNetwork(inputs,n_hidden_layers=5,n_neurons=100,name=None,\n",
    "                      activation=tf.nn.elu,initializer=he_init):\n",
    "    with tf.variable_scope(name,\"DNN\"):\n",
    "        for i in range(n_hidden_layers):\n",
    "            layers1 = tf.layers.dense(inputs, n_neurons, activation=activation, kernel_initializer=initializer,name=\"hidden%d\" % (i + 1))\n",
    "        return layers1\n",
    "\n",
    "\n",
    "n_inputs = 28*28\n",
    "n_outputs = 10\n",
    "    \n",
    "#Now create variable place holders to store the data\n",
    "X = tf.placeholder(tf.float32, shape=(None,n_inputs),name=\"X\")\n",
    "y = tf.placeholder(tf.int16, shape=(None),name=\"y\")\n",
    "\n",
    "#Now use the deep neural network function on the placeholder X\n",
    "dnn_outputs = DeepNeuralNetwork(X)\n",
    "#now specify a logistic classifier output\n",
    "logits = tf.layers.dense(dnn_outputs, n_outputs,kernel_initializer=he_init,name=\"logits\")\n",
    "#now specify the activation output\n",
    "y_proba = tf.nn.softmax(logits,name='y_proba')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 8 part b\n",
    "\n",
    "Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4.\n",
    "Use softmax output with five neurons\n",
    "\n",
    "So the previous network worked with 5 layers - 5 different classes were used.\n",
    "\n",
    "In this network, we should train the previous network on images 0-4, (using 5 layers), and for the next one, build a model of 10 layers, where we pre-load the first 5.\n",
    "\n",
    "To start off, lets get neural network to train and predict the 0-4 images successfully using tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]\n",
    "\n",
    "X_train1 = X_train[y_train < 5]\n",
    "y_train1 = y_train[y_train < 5]\n",
    "X_valid1 = X_valid[y_valid < 5]\n",
    "y_valid1 = y_valid[y_valid < 5]\n",
    "X_test1 = X_test[y_test < 5]\n",
    "y_test1 = y_test[y_test < 5]\n",
    "\n",
    "\n",
    "X_train2 = X_train[y_train >= 5]\n",
    "y_train2 = y_train[y_train >= 5]\n",
    "X_test2 = X_test[y_test >= 5]\n",
    "y_test2 = y_test[y_test >=5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first function we did a few things. We Created a function that returns a few objects, namely;\n",
    "* The probability of each element being in a specified class\n",
    "* The loss function\n",
    "* The training operation\n",
    "* The initializer object\n",
    "* The saver object\n",
    "* Loss summary text\n",
    "\n",
    "This function then specifies\n",
    "* The input and target data\n",
    "* The number of layers and neurons in each\n",
    "* The activation function for each layer\n",
    "* The initializer for the data points\n",
    "* The learning rate\n",
    "\n",
    "This function is structured by:\n",
    "* a name scope \"Multi_class\" to describe all the objects on the graph\n",
    "* a name scope for the \"model\" operation, which includes the layers, initialization and final layer\n",
    "    * The layers, which are created using a loop\n",
    "    * The final layer, specifying the inputs from the other layer and the outputs from the others\n",
    "    * The initializer and activation function are specified in each layer object\n",
    "    * It created y_proba, the probability of being a specific class, which is returns\n",
    "* a name scope for \"train\" operation, which specifies\n",
    "    * The cross entropy loss function and reducing its mean\n",
    "    * The optimization of this loss function using Adam optimization\n",
    "    * The training operation which puts the opitmizer into action in order to minimize the loss function\n",
    "    * The loss summary, which is a string describing the loss function at each iteration\n",
    "* a name scope for global variable initialization\n",
    "* a name scope for the saver object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#He initialization\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "he_init = tf.variance_scaling_initializer()\n",
    "def multi_classifier(X,y, initializer=he_init, activation=tf.nn.elu, seed=42, learning_rate=0.01):\n",
    "    n_inputs = int(X.get_shape()[1])\n",
    "    n_outputs = 10\n",
    "    \n",
    "    n_hidden1 = 300\n",
    "    n_hidden2 = 50\n",
    "    n_hidden3 = 50\n",
    "    n_hidden4 = 50\n",
    "    n_hidden5 = 50\n",
    "    n_outputs = 10\n",
    "    with tf.name_scope(\"Multi_class\"):\n",
    "        \n",
    "        with tf.name_scope(\"Model\"):\n",
    "            if initializer == None:\n",
    "                initializer = tf.random_uniform([n_inputs,1],-1,1,seed=seed)\n",
    "    \n",
    "            \n",
    "            hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "            hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "            hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "            hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "            hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "            logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "            y_proba = tf.nn.softmax(logits,name=\"softmax_classification\")\n",
    "\n",
    "\n",
    "                \n",
    "        with tf.name_scope(\"Train\"):\n",
    "            \n",
    "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "            loss = tf.reduce_mean(xentropy,name=\"cross_entropy_loss\")\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            training_op = optimizer.minimize(loss,name=\"training_op\")\n",
    "            loss_summary = tf.summary.scalar(\"loss\",loss)\n",
    "        \n",
    "        with tf.name_scope(\"evaluate\"):\n",
    "            correct = tf.nn.in_top_k(logits,y,1)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32),name=\"accuracy\")\n",
    "        \n",
    "            \n",
    "        with tf.name_scope(\"init\"):\n",
    "            init = tf.global_variables_initializer()\n",
    "            \n",
    "        with tf.name_scope(\"Save\"):\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "        return y_proba, loss, training_op, init, saver, loss_summary, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now initialize the inputs\n",
    "\n",
    "n_inputs = X_train.shape[1]\n",
    "#He initialization\n",
    "he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None,n_inputs),name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "y_proba, loss, training_op, init, saver, loss_summary, accuracy = multi_classifier(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "X_op,y_op = X_train1,y_train1\n",
    "\n",
    "def get_batch_size_and_number(X,y):\n",
    "    r = len(set(y))\n",
    "    #let the size of each batch be the average number of rows corresponding to each class\n",
    "    size = int(np.mean([X[y==i].shape[0] for i in range(r)]))\n",
    "    n_batches = int(np.ceil(X.shape[0]/size))\n",
    "    return size, n_batches\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size, n_batches = get_batch_size_and_number(X_op,y_op)\n",
    "\n",
    "#now we need a function that gets a random batch. What is the purpose of this function?\n",
    "# The purpose of the function is to return a random sequence of rows the size of our batch\n",
    "\n",
    "def random_batch(X,y,size):\n",
    "    indices = random.sample(range(1, X.shape[0]), size)\n",
    "    X_batch,y_batch = X[indices],y[indices]\n",
    "    return X_batch,y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss val:  0.60017353  Accuracy:  0.77651757\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime as dt\n",
    "import os\n",
    "X_test_N, y_test_N = X_test1, y_test1\n",
    "n_epochs = 200\n",
    "def directory(prefix=\"\"):\n",
    "    now = dt.utcnow().strftime(\"%Y%m%d%H%m%M%S\")\n",
    "    root_logdir = \"tf_multi_class_logging\"\n",
    "    if prefix:\n",
    "        prefix +=\"-\"\n",
    "    name = prefix + \"run-\"+now\n",
    "    return \"{}/{}/\".format(root_logdir,name)\n",
    "    \n",
    "\n",
    "file_writer = tf.summary.FileWriter(directory(),tf.get_default_graph())\n",
    "\n",
    "# now we make checkpoints, for each epoch and for the final model\n",
    "checkpoint_path = \"./tmp2/multi_classifier_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path +\".epoch\"\n",
    "final_model_path = \"./tmp2/multi_classifier_model_final.ckpt\"\n",
    "\n",
    "#Create a session we want to save to\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    #if os.path.isfile(checkpoint_epoch_path):\n",
    "        # if the checkpoint file exists, restore the model and load epoch number\n",
    "     #   with open(checkpoint_epoch_path,\"rb\") as f:\n",
    "      #      start_epoch = int(f.read())\n",
    "      #  print(\"Training was interrupted, continuing at epoch: \", start_epoch)\n",
    "       # saver.restore(sess, checkpoint_path)\n",
    "    \n",
    "    #else:\n",
    "    start_epoch = 0\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = random_batch(X_op,y_op, batch_size)\n",
    "            sess.run(training_op, feed_dict={X:X_batch,y:y_batch})\n",
    "        loss_val,summary_str,acc_val = sess.run([loss,loss_summary,accuracy], feed_dict={X:X_op,y:y_op})\n",
    "        file_writer.add_summary(summary_str, epoch)\n",
    "        #y_proba_val = sess.run([y_proba], feed_dict={X:X_test_N,y:y_test_N})\n",
    "        \n",
    "        #save at every 5% of total epochs number - so 21 saves total.\n",
    "        if (epoch % n_epochs*(0.05)==0):\n",
    "            print(\"Epoch: \",epoch,\"Loss val: \",loss_val,\" Accuracy: \",acc_val)\n",
    "            saver.save(sess,checkpoint_epoch_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\"%(epoch+1))\n",
    "                \n",
    "    #at the end of the epochs, save the final model path.\n",
    "    saver.save(sess, final_model_path)\n",
    "    \n",
    "    #now evaluate the probability of y_proba\n",
    "    final_accuracy = accuracy.eval(feed_dict={X:X_test_N,y:y_test_N})\n",
    "    print(\"Test set evaluation of accuracy: \",final_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp2/multi_classifier_model.ckpt.epoch'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_epoch_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "n=10\n",
    "for i in range(n):\n",
    "    if (i%(0.05*n))==0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.05*n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
