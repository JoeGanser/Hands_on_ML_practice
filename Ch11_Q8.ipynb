{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a DNN with 5 hidden layers of 100 neurons each, He initialization, and Elu activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#now specify he initialization\n",
    "\n",
    "he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "def dnn(inputs, n_hidden_layers=5, n_neurons=100, name=None, activation=tf.nn.elu, initializer=he_init):\n",
    "    #now specify a name scope\n",
    "    with tf.variable_scope(name, \"dnn\"):\n",
    "        for layer in range(n_hidden_layers):\n",
    "            inputs = tf.layers.dense(inputs,n_neurons, activation=activation, kernel_initializer=initializer,\n",
    "                                    name=\"hidden%d\" %(layer +1))\n",
    "        return inputs\n",
    "    \n",
    "#now specify the inputs, for MNIST data\n",
    "\n",
    "n_inputs = 28*28\n",
    "n_outputs = 5\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "# make a final output layer using the function above\n",
    "logits = tf.layers.dense(dnn(X), n_outputs, kernel_initializer=he_init, name=\"logits\")\n",
    "Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Adam optimization AND early stopping, train the MNIST but only on digits 0 to 4, as we will use transfer learning to train digits 5 to 9. You will need a softmax output layer with 5 neurons.\n",
    "Make sure to save checkpoints at regular intervals and save the final model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideOutput": true
   },
   "outputs": [],
   "source": [
    "#specify the learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "#specify the loss function setup (Cross entropy plus reducing the mean)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "#specify the optimizer that learns at the learning rate\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "#specify what that optimizer acts on (the loss function)\n",
    "training_op = optimizer.minimize(loss, name=\"training_op\")\n",
    "\n",
    "#identify when the function is correct\n",
    "correct = tf.nn.in_top_k(logits,y,1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "#specify the initializer\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#specify the saver\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]\n",
    "\n",
    "X_train1 = X_train[y_train < 5]\n",
    "y_train1 = y_train[y_train < 5]\n",
    "X_valid1 = X_valid[y_valid < 5]\n",
    "y_valid1 = y_valid[y_valid < 5]\n",
    "X_test1 = X_test[y_test < 5]\n",
    "y_test1 = y_test[y_test < 5]\n",
    "\n",
    "\n",
    "X_train2 = X_train[y_train>=5]\n",
    "y_train2 = y_train[y_train>=5]\n",
    "X_test2 = X_test[y_test>=5]\n",
    "y_test2 = y_test[y_test>=5]\n",
    "X_valid2 = X_valid[y_valid>=5]\n",
    "y_valid2 = y_valid[y_valid>=5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "21041\n",
      "1\n",
      "12587\n",
      "\n",
      "\n",
      "[21041 12587  5510 26325 10110  5152 10993  8870  7952 21996 18137 14908\n",
      "  6890 26632  7038  1438  1148 16506 20984 11532  3446]\n"
     ]
    }
   ],
   "source": [
    "rnd_idx = np.random.permutation(len(X_train1))\n",
    "for rnd_indices in np.array_split(rnd_idx, len(X_train1)//20)[:1]:\n",
    "    print('0')\n",
    "    print(rnd_indices[0])\n",
    "    print('1')\n",
    "    print(rnd_indices[1])\n",
    "    print('\\n')\n",
    "    print(rnd_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping!\n",
      "23\tValidation loss:1.768096\tBest loss: 0.111777\tAccuracy: 19.08%\n"
     ]
    }
   ],
   "source": [
    "#specify the number of epochs and the batch size\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "#specify the number of max checks without progress\n",
    "max_checks_without_progress = 20\n",
    "#initialize the checks without progress and best loss function value\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train1))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train1)//batch_size):\n",
    "            X_batch, y_batch = X_train1[rnd_indices], y_train1[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X:X_batch,y:y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy],feed_dict={X:X_valid1, y: y_valid1})\n",
    "        \n",
    "        if loss_val < best_loss:\n",
    "            save_path = saver.save(sess, \"./my_mnist_model_0_to_4.ckpt\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            \n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "    print(\"{}\\tValidation loss:{:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(epoch, loss_val, best_loss, acc_val*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_mnist_model_0_to_4.ckpt\n",
      "Final test accuracy: 98.02%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_mnist_model_0_to_4.ckpt\")\n",
    "    acc_test = accuracy.eval(feed_dict={X:X_test1, y: y_test1})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next exercise: tune the hyper parameters using cross validation and see what precision you can achieve.\n",
    "\n",
    "Let's create a DNNClassifier class, compatible with scikitlearns RandomizedSearchCV class to perform hyperparameter tuning. Here are the key points of this implementation\n",
    "\n",
    "* The init(), `__init__()` method (constructor) does nothing more than create instance variables for each of the hyperparameters.\n",
    "\n",
    "* The `fit()` method creates the graphg, starts a session and trains the model:\n",
    "\n",
    "    * It calls the `_build_graph()` method, to build the graph (much like the graph we defined earlier). Once this method is done creating the graph, it saves all the important operations as instance variables for easy access by other methods.\n",
    "    \n",
    "    * The `_dnn()` method builds the hidden layers, just like the `dnn()` function above, but also with support for *batch normalization* and drop out (for the next exercises).\n",
    "    \n",
    "    * If the `.fit()` method is given a validation set; `X_valid, y_valid` then it implements early stopping. This implementation does not save the best model to disk, but rather to memory: it uses the `_get_model_params()` method to get all the graph's variables and their values, and the `_restore_model_params()` method to restore the variable values (of the best model found). This trick helps speed up training.\n",
    "    \n",
    "    * After the `.fit()` method has finished training the model, it keeps the session open so that predictions can be made quickly without having to save a model to disk and restore for every prediction. You can close the session by calling the `.close_session()` method.\n",
    "    \n",
    " * The `predict_proba()` method uses the trained model to predict the class probabilities. \n",
    " \n",
    " * The `predict()` method calls the `predict_proba()` and returns the class with the highest probability, for each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "class DNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self,n_hidden_layers=5, n_neurons=100, optimizer_class = tf.train.AdamOptimizer, \n",
    "                 learning_rate=0.01, batch_size=20, activation=tf.nn.elu, initializer=he_init,\n",
    "                batch_norm_momentum=None, dropout_rate=None, random_state=None):\n",
    "        '''Initialize the DNNClassifier by simply storing all the hyper-parameters'''\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.initializer = initializer\n",
    "        self.batch_norm_momentum = batch_norm_momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.random_state = random_state\n",
    "        self._session = None\n",
    "    \n",
    "    def _dnn(self, inputs):\n",
    "        '''Build the hidden layers, with support for batch normalization and dropout.'''\n",
    "        for layer in range(self.n_hidden_layers):\n",
    "            if self.dropout_rate:\n",
    "                inputs = tf.layers.dropout(inputs, self.dropout_rate, training=self._training)\n",
    "            if self.batch_norm_momentum:\n",
    "                inputs = tf.layers.batch_normalization(inputs, momentum=self.batch_norm_momentum)\n",
    "        inputs = self.activation(inputs, name=\"hidden%d_out\"%(layer+1))\n",
    "        return inputs\n",
    "    \n",
    "    def _build_graph(self, n_inputs, n_outputs):\n",
    "        '''Build the same model as earlier.'''\n",
    "        if self.random_state is not None:\n",
    "            tf.set_random_seed(self.random_state)\n",
    "            np.random.seed(self.random_state)\n",
    "        \n",
    "        X = tf.placeholder(tf.float32, shape=(None, n_inputs),name=\"X\")\n",
    "        y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "        \n",
    "        if self.batch_norm_momentum or self.dropout_rate:\n",
    "            self._training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "        \n",
    "        else:\n",
    "            self._training = None\n",
    "        \n",
    "        dnn_outputs = self._dnn(X)\n",
    "        \n",
    "        logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init, name=\"logits\")\n",
    "        \n",
    "        y_proba = tf.nn.softmax(logits, name=\"y_proba\")\n",
    "        \n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "        \n",
    "        loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "        \n",
    "        optimizer = self.optimizer_class(learning_rate=self.learning_rate)\n",
    "        \n",
    "        training_op = optimizer.minimize(loss)\n",
    "        \n",
    "        correct = tf.nn.in_top_k(logits,y,1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct,tf.float32, name=\"accuracy\"))\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        #make the important operations available easily through instance variables\n",
    "        \n",
    "        self._X, self._y = X, y\n",
    "        self._Y_proba, self._loss = Y_proba, loss\n",
    "        self._training_op, self._accuracy = training_op, accuracy\n",
    "        self._init, self._saver = init, saver\n",
    "    \n",
    "    def close_session(self):\n",
    "        if self._session:\n",
    "            self._session.close()\n",
    "    \n",
    "    def _get_model_params(self):\n",
    "        \"\"\"Get all variable values (used for early stopping, faster than saving to disk)\"\"\"\n",
    "        with self._graph.as_default():\n",
    "            gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        return {gvar.op.name: value for gvar, value in zip(gvars, self._session.run(gvars))}\n",
    "    \n",
    "    def _restore_model_params(self, model_params):\n",
    "        \"\"\"Set all variables to the given values for early stopping, faster than loading from disk\"\"\"\n",
    "        gvar_names = list(model_params.keys())\n",
    "        \n",
    "        assign_ops = {gvar_name: self._graph.get_operation_by_name(gvar_name +\"/Assign\") for gvar_name in gvar_names}\n",
    " \n",
    "        init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "    \n",
    "        feed_dict = {init_values[gvar_name]:model_params[gvar_name] for gvar_name in gvar_names}\n",
    "        \n",
    "        self._session.run(assign_ops, feed_dict=feed_dict)\n",
    "        \n",
    "    def fit(self, X,y,n_epochs=100, X_valid=None, y_valid=None):\n",
    "        \"\"\"Fit the model to the training set. If X_valid and y_valid are provided, use early stopping.\"\"\"\n",
    "        self.close_session()\n",
    "        \n",
    "        #infer n_inputs and n_outputs from the training set\n",
    "        n_inputs = X.shape[1]\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_outputs = len(self.classes_)\n",
    "        \n",
    "        #translate the labels vector to a vector of sorted class indices, containing \n",
    "        # intefers from 0 to n_outputs - 1\n",
    "        self.classes_to_index_ = {label:index for index, label in enumerate(self.classes_)}\n",
    "        \n",
    "        y = np.array([self.classes_to_index_[label] for label in y], dtype=np.int32)\n",
    "        \n",
    "        self._graph = tf.Graph()\n",
    "        \n",
    "        with self._graph.as_default():\n",
    "            self._build_graph(n_inputs, n_outputs)\n",
    "            #EXTRA OPS FOR BATCH NORMALIZATION\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys().UPDATE_OPS)\n",
    "        \n",
    "        #mneeded in case of early stopping\n",
    "        max_checks_without_progress = 20\n",
    "        \n",
    "        #initialize checks without progress\n",
    "        checks_without_progress = 0\n",
    "        \n",
    "        #initialize the best loss and best params\n",
    "        best_loss = np.infty\n",
    "        best_params = None\n",
    "        \n",
    "        #Now train the model!\n",
    "        self._session = tf.Session(graph=self._graph)\n",
    "        with self._session.as_default() as sess:\n",
    "            self._init.run()\n",
    "            for epoch in range(n_epochs):\n",
    "                rnd_idx = np.random.permutation(len(X))\n",
    "                for rnd_indices in np.array_split(rnd_idx, len(X)//self.batch_size):\n",
    "                    X_batch, y_batch = X[rnd_indices], y[rnd_indices]\n",
    "                    feed_dict = {self._X: X_batch, self._y:y_batch}\n",
    "                    \n",
    "                    if self._training is not None:\n",
    "                        feed_dict[self._training] = True\n",
    "                    sess.run(extra_update_ops, feed_dict=feed_dict)\n",
    "                \n",
    "                if (X_valid is not None) and (y_valid is not None):\n",
    "                    loss_val, acc_val = sess.run([self._loss, self._accuracy],\n",
    "                                                 feed_dict={self._X:X_valid, self._y:y_valid})\n",
    "                    \n",
    "                    if loss_val < best_loss:\n",
    "                        best_params = self._get_model_params()\n",
    "                        best_loss = loss_val\n",
    "                        checks_without_progress = 0\n",
    "                    else:\n",
    "                        checks_without_progress+=1\n",
    "\n",
    "                    message123 = \"{}\\tValidation loss:{:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(epoch, loss_val, \n",
    "                                                                                                           best_loss, acc_val*100)\n",
    "                    print(message123)\n",
    "                    \n",
    "                    if checks_without_progress>max_checks_without_progress:\n",
    "                        print(\"Early Stopping!\")\n",
    "                        break\n",
    "                else:\n",
    "                    loss_train, acc_train = sess.run([self._loss, self._accuracy], feed_dict={self._X:X_batch,\n",
    "                                                                                              self._y:y_batch})\n",
    "                    print(\"{}\\tLast training batch loss: {:.6f}\\t Accuracy: {:.2f}%\".format(epoch, loss_train,\n",
    "                                                                                           acc_train*100))\n",
    "            if best_params:\n",
    "                self._restore_model_params(best_params)\n",
    "            return self\n",
    "        \n",
    "        def predict_proba(self, X):\n",
    "            if not self._session:\n",
    "                raise NotFittedError(\"This %s instance is not fitted yet\" %self.__class__.__name__)\n",
    "            with self._session.as_default() as sess:\n",
    "                return self._Y_proba.eval(feed_dict={self._X: X})\n",
    "            \n",
    "        \n",
    "        def predict(self,X):\n",
    "            class_indices = np.argmax(self.predict_proba(X),axis=1)\n",
    "            return np.array([[self.classes_[class_index]] for class_index in class_indices], np.int32)\n",
    "        \n",
    "        def save(self, path):\n",
    "            self._saver.save(self._session, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss:1.672759\tBest loss: 1.672759\tAccuracy: 14.39%\n",
      "1\tValidation loss:1.672759\tBest loss: 1.672759\tAccuracy: 14.39%\n",
      "2\tValidation loss:1.672759\tBest loss: 1.672759\tAccuracy: 14.39%\n",
      "3\tValidation loss:1.672759\tBest loss: 1.672759\tAccuracy: 14.39%\n",
      "4\tValidation loss:1.672759\tBest loss: 1.672759\tAccuracy: 14.39%\n",
      "5\tValidation loss:1.672759\tBest loss: 1.672759\tAccuracy: 14.39%\n",
      "6\tValidation loss:1.672759\tBest loss: 1.672759\tAccuracy: 14.39%\n",
      "7\tValidation loss:1.672759\tBest loss: 1.672759\tAccuracy: 14.39%\n",
      "8\tValidation loss:1.672759\tBest loss: 1.672759\tAccuracy: 14.39%\n",
      "9\tValidation loss:1.672759\tBest loss: 1.672759\tAccuracy: 14.39%\n",
      "10\tValidation loss:1.672759\tBest loss: 1.672759\tAccuracy: 14.39%\n",
      "11\tValidation loss:1.672759\tBest loss: 1.672759\tAccuracy: 14.39%\n",
      "12\tValidation loss:1.672759\tBest loss: 1.672759\tAccuracy: 14.39%\n",
      "13\tValidation loss:1.672759\tBest loss: 1.672759\tAccuracy: 14.39%\n",
      "14\tValidation loss:1.672759\tBest loss: 1.672759\tAccuracy: 14.39%\n",
      "15\tValidation loss:1.672759\tBest loss: 1.672759\tAccuracy: 14.39%\n",
      "16\tValidation loss:1.672759\tBest loss: 1.672759\tAccuracy: 14.39%\n",
      "17\tValidation loss:1.672759\tBest loss: 1.672759\tAccuracy: 14.39%\n",
      "18\tValidation loss:1.672759\tBest loss: 1.672759\tAccuracy: 14.39%\n",
      "19\tValidation loss:1.672759\tBest loss: 1.672759\tAccuracy: 14.39%\n",
      "20\tValidation loss:1.672759\tBest loss: 1.672759\tAccuracy: 14.39%\n",
      "21\tValidation loss:1.672759\tBest loss: 1.672759\tAccuracy: 14.39%\n",
      "Early Stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(activation=<function elu at 0x126668f28>,\n",
       "       batch_norm_momentum=None, batch_size=20, dropout_rate=None,\n",
       "       initializer=<tensorflow.python.ops.init_ops.VarianceScaling object at 0x183fec6198>,\n",
       "       learning_rate=0.01, n_hidden_layers=5, n_neurons=100,\n",
       "       optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "       random_state=42)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf = DNNClassifier(random_state=42)\n",
    "dnn_clf.fit(X_train1,y_train1, n_epochs=1000, X_valid=X_valid1,y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
